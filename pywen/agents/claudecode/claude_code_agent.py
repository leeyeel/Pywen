"""
Claude Code Agent - Python implementation of the Claude Code assistant
"""
import os
from typing import Dict, List, Optional, AsyncGenerator, Any
import datetime
from rich import print
from pywen.agents.base_agent import BaseAgent
from pywen.tools.base import BaseTool
from pywen.utils.llm_basics import LLMMessage, LLMResponse
from pywen.utils.tool_basics import ToolCall, ToolResult
from pywen.core.trajectory_recorder import TrajectoryRecorder
from .prompts import ClaudeCodePrompts
from .context_manager import ClaudeCodeContextManager
from pywen.core.session_stats import session_stats

# Import memory moniter and file restorer
from pywen.memory.memory_moniter import MemoryMonitor, AdaptiveThreshold
from pywen.memory.file_restorer import IntelligentFileRestorer



class ClaudeCodeAgent(BaseAgent):
    """Claude Code Agent implementation"""

    def __init__(self, config, cli_console=None):
        super().__init__(config, cli_console)
        self.type = "ClaudeCodeAgent"
        self.prompts = ClaudeCodePrompts()
        self.project_path = os.getcwd()
        self.max_iterations = getattr(config, 'max_iterations', 10)

        # Initialize context manager
        self.context_manager = ClaudeCodeContextManager(self.project_path)
        self.context = {}

        # Initialize conversation history for session continuity
        self.conversation_history: List[LLMMessage] = []
        # self.max_history_messages = getattr(config, 'max_history_messages', 20)  # Keep last 20 messages

        # Ensure trajectories directory exists
        from pywen.config.loader import get_trajectories_dir
        trajectories_dir = get_trajectories_dir()

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        trajectory_path = trajectories_dir / f"claude_code_trajectory_{timestamp}.json"
        self.trajectory_recorder = TrajectoryRecorder(trajectory_path)

        # Setup Claude Code specific tools after base tools
        self._setup_claude_code_tools()

        self.tools = self.tool_registry.list_tools()

        #self._update_context()

        # Register this agent with session stats
        session_stats.set_current_agent(self.type)

        # Initialize memory moniter and file restorer
        self.current_turn = 0
        self.memory_moniter = MemoryMonitor(AdaptiveThreshold(check_interval=3, max_tokens=200000, rules=((0.92, 1), (0.80, 1), (0.60, 2), (0.00, 3))))
        self.file_restorer = IntelligentFileRestorer()

    def _setup_claude_code_tools(self):
        """Setup Claude Code specific tools and configure them."""
        # Import agent registry
        from pywen.core.agent_registry import get_agent_registry
        agent_registry = get_agent_registry()

        # Configure task_tool and architect_tool with agent registry
        task_tool = self.tool_registry.get_tool('task_tool')
        if task_tool and hasattr(task_tool, 'set_agent_registry'):
            task_tool.set_agent_registry(agent_registry)

        architect_tool = self.tool_registry.get_tool('architect_tool')
        if architect_tool and hasattr(architect_tool, 'set_agent_registry'):
            architect_tool.set_agent_registry(agent_registry)

    def get_enabled_tools(self) -> List[str]:
        """Return list of enabled tool names for Claude Code Agent."""
        return [
            'read_file', 'write_file', 'edit_file', 'read_many_files',
            'ls', 'grep', 'glob', 'bash', 'web_fetch', 'web_search','task_tool','architect_tool','think','todo_write'
        ]

    def _build_system_prompt(self) -> str:
        """Build system prompt with context and tool descriptions."""
        return self.prompts.get_system_prompt(self.context)

    def _update_context(self):
        """
        Update the context information using the Context Manager
        """
        try:
            # Use context manager to get comprehensive context
            self.context = self.context_manager.get_context()

            # Use prompts to build additional context
            additional_context = self.prompts.build_context(self.project_path)
            self.context.update(additional_context)

        except Exception as e:
            if self.cli_console:
                self.cli_console.print(f"Failed to build context: {e}", "yellow")
            # Fallback to minimal context
            self.context = {'project_path': self.project_path}

    def clear_conversation_history(self):
        """
        Clear the conversation history (useful for starting fresh)
        """
        self.conversation_history.clear()
        if self.cli_console:
            self.cli_console.print("Conversation history cleared", "green")

    def get_conversation_summary(self) -> str:
        """
        Get a summary of the current conversation history
        """
        if not self.conversation_history:
            return "No conversation history"

        user_messages = len([msg for msg in self.conversation_history if msg.role == "user"])
        assistant_messages = len([msg for msg in self.conversation_history if msg.role == "assistant"])
        tool_messages = len([msg for msg in self.conversation_history if msg.role == "tool"])

        return f"Conversation: {user_messages} user, {assistant_messages} assistant, {tool_messages} tool messages"

    async def run(self, query: str, **kwargs) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Main execution loop for Claude Code Agent
        Entry point that sets up initial context and calls the recursive query function
        """
        try:
            # Record the cunrrent turn
            self.current_turn += 1

            # Set this agent as current in the registry for tool access
            from pywen.core.agent_registry import set_current_agent
            set_current_agent(self)

            # Start trajectory recording
            self.trajectory_recorder.start_recording(
                task=query,
                provider=self.config.model_config.provider.value,
                model=self.config.model_config.model,
                max_steps=self.max_iterations
            )

            # Record task start in session stats
            session_stats.record_task_start(self.type)


            yield {"type": "user_message", "data": {"message": query}}
            # Update context before each run
            self._update_context()

            # Build system prompt with context
            system_prompt = self.prompts.get_system_prompt(self.context)

            # Add new user message to conversation history
            user_message = LLMMessage(role="user", content=query)
            self.conversation_history.append(user_message)

            # Manage conversation history size
            # self._manage_conversation_history()

            # Initialize conversation with system prompt and full history
            messages = [LLMMessage(role="system", content=system_prompt)] + self.conversation_history.copy()

            # Start recursive query loop with depth control
            async for event in self._query_recursive(messages, system_prompt, is_root=True, depth=0, **kwargs):
                yield event

            # # End trajectory recording
            # self.trajectory_recorder.end_recording(success=True, final_result="Task completed")

        except Exception as e:
            yield {
                "type": "error",
                "data":{
                "error": f"Agent error: {str(e)}"
                },
            }

    async def _query_recursive(
        self,
        messages: List[LLMMessage],
        system_prompt: str,
        depth: int = 0,
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Recursive query function - implements the core query loop from original Claude Code
        This function calls itself recursively when tool calls are present

        Args:
            messages: Conversation history
            system_prompt: System prompt
            depth: Current recursion depth (for max_iterations control)
            **kwargs: Additional arguments
        """
        try:
            # TRAJECTORY: Record recursion start


            # ðŸ”¢ DEPTH CONTROL: Check max iterations
            if depth >= self.max_iterations:
                yield {
                    "type": "max_turns_reached",
                    "data": {
                        "max_iterations": self.max_iterations,
                        "current_depth": depth
                    }
                }
                return

            # Check for abort signal
            if kwargs.get('abort_signal') and kwargs['abort_signal'].is_set():
                yield {
                    "type": "error",
                    "data": {"error": "Operation was cancelled"}
                }
                return


            # Get assistant response with fine-grained streaming events
            assistant_message, tool_calls = None, []
            async for response_event in self._get_assistant_response_streaming(messages, depth=depth, **kwargs):
                if response_event["type"] in ["llm_stream_start", "llm_chunk", "content"]:
                    # Forward streaming events to caller
                    yield response_event
                elif response_event["type"] == "assistant_response":
                    # Extract final response
                    assistant_message = response_event["assistant_message"]
                    tool_calls = response_event["tool_calls"]
                    final_response = response_event.get("final_response")

            # ðŸ“ TRAJECTORY: Record LLM interaction
            if assistant_message:
                # Add assistant message to conversation history
                self.conversation_history.append(assistant_message)

                # Create LLMResponse object for trajectory recording with usage info
                llm_response = LLMResponse(
                    content=assistant_message.content or "",
                    tool_calls=[ToolCall(
                        call_id=tc.get("id", "unknown"),
                        name=tc.get("name", ""),
                        arguments=tc.get("arguments", {})
                    ) for tc in tool_calls] if tool_calls else None,
                    model=self.config.model_config.model,
                    finish_reason="stop",
                    usage=final_response.usage if final_response and hasattr(final_response, 'usage') else None
                )

                # è®°å½•LLMäº¤äº’ (session stats ä¼šåœ¨ trajectory_recorder ä¸­è‡ªåŠ¨è®°å½•)
                self.trajectory_recorder.record_llm_interaction(
                    messages=messages,
                    response=llm_response,
                    provider=self.config.model_config.provider.value,
                    model=self.config.model_config.model,
                    tools=self.tools,
                    current_task=f"Processing query at depth {depth}",
                    agent_name=self.type
                )


            # TOP CONDITION: No tool calls means we're done
            if not tool_calls:

                # Run memory moniter after Task completed
                current_usage = final_response.usage.total_tokens
                comression = await self.memory_moniter.run_monitored(self.current_turn, self.conversation_history, current_usage)
                if comression is not None:
                    self.conversation_history = comression

                # Yield task completion event
                yield {
                    "type": "task_complete",
                    "content": assistant_message.content if assistant_message else "",
                    
                }

                return

            # Yield tool call events for each tool
            for tool_call in tool_calls:
                yield {
                    "type": "tool_call_start",
                    "data": {
                        "call_id": tool_call.get("id", "unknown"),
                        "name": tool_call["name"],
                        "arguments": tool_call.get("arguments", {})
                    }
                }

            # Execute tools and get results (simplified)
            tool_results = []
            async for tool_event in self._execute_tools(tool_calls, **kwargs):
                # Forward all tool events to caller
                yield tool_event
                
                if tool_event["type"] == "tool_results":
                    # Extract final results
                    tool_results = tool_event["results"]

            # Check for abort signal after tool execution
            if kwargs.get('abort_signal') and kwargs['abort_signal'].is_set():
                yield {
                    "type": "error",
                    "data": {"error": "Operation was cancelled during tool execution"}
                }
                return

            # Add tool results to conversation history
            for tool_result in tool_results:
                self.conversation_history.append(tool_result)
            

            # ðŸ”„ RECURSIVE CALL: Use the updated conversation history
            # Rebuild messages from system prompt + current conversation history
            updated_messages = [
                LLMMessage(role="system", content=system_prompt)
            ] + self.conversation_history.copy()

            # ðŸ”„ RECURSIVE CALL: Continue with updated message history and incremented depth
            async for event in self._query_recursive(updated_messages, system_prompt, depth=depth+1, **kwargs):
                yield event

        except Exception as e:
            yield {
                "type": "error",
                "data":{"error": f"Query error: {str(e)}"},
                
            }

    async def _get_assistant_response_streaming(
        self,
        messages: List[LLMMessage],
        depth: int = 0,
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Get assistant response from LLM with fine-grained streaming events
        Yields: llm_stream_start, llm_chunk, assistant_response events
        """
        try:
            # Check for abort signal
            if kwargs.get('abort_signal') and kwargs['abort_signal'].is_set():
                yield {
                    "type": "error",
                    "data": {"error": "Operation was cancelled"}
                }
                return
            response_stream = await self.llm_client.generate_response(
                messages=messages,
                tools=self.tools,
                stream=True
            )

            # Yield stream start event
            yield {
                "type": "llm_stream_start",
                "data": {"depth": depth}
            }

            # 1. æµå¼å¤„ç†å“åº”ï¼Œæ”¶é›†å·¥å…·è°ƒç”¨
            final_response = None
            previous_content = ""
            collected_tool_calls = []

            async for response_chunk in response_stream:
                final_response = response_chunk

                # å‘é€å†…å®¹å¢žé‡
                if response_chunk.content:
                    current_content = response_chunk.content
                    if current_content != previous_content:
                        new_content = current_content[len(previous_content):]
                        if new_content:
                            yield {
                                "type": "llm_chunk",
                                "data": {"content": new_content}
                            }
                        previous_content = current_content

                # æ”¶é›†å·¥å…·è°ƒç”¨ï¼ˆä¸ç«‹å³æ‰§è¡Œï¼‰
                if response_chunk.tool_calls:
                    collected_tool_calls.extend(response_chunk.tool_calls)

            # 2. æµç»“æŸåŽå¤„ç†
            if final_response:
                # æ·»åŠ åˆ°å¯¹è¯åŽ†å²
                assistant_msg = LLMMessage(
                    role="assistant",
                    content=final_response.content,
                    tool_calls=final_response.tool_calls
                )

                # ç®€åŒ–çš„å·¥å…·è°ƒç”¨æ ¼å¼è½¬æ¢
                tool_calls = []
                if final_response.tool_calls:
                    for tc in final_response.tool_calls:
                        tool_calls.append({
                            "id": tc.call_id,
                            "name": tc.name,
                            "arguments": tc.arguments
                        })

                # è¿”å›žæœ€ç»ˆçš„assistant_responseäº‹ä»¶ï¼ŒåŒ…å«usageä¿¡æ¯
                yield {
                    "type": "assistant_response",
                    "assistant_message": assistant_msg,
                    "tool_calls": tool_calls,
                    "final_response": final_response  # åŒ…å«å®Œæ•´çš„å“åº”å¯¹è±¡ï¼ŒåŒ…æ‹¬usage
                }

        except Exception as e:
            yield {
                "type": "error",
                "data": {"error": f"Streaming failed, falling back to non-streaming: {str(e)}"}
            }




    async def _execute_tools(
        self,
        tool_calls: List[Dict[str, Any]],
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Simplified tool execution with smart concurrency
        """
        if not tool_calls:
            yield {
                "type": "tool_results",
                "results": []
            }
            return

        # Determine if tools can run concurrently
        can_run_concurrently = all(self._is_tool_readonly(tc["name"]) for tc in tool_calls)

        if can_run_concurrently and len(tool_calls) > 1:
            # Execute read-only tools concurrently
            yield {"type": "tool_execution", "strategy": "concurrent"}
            tool_results = await self._execute_concurrent_tools(tool_calls, **kwargs)
        else:
            # Execute tools serially (safer for write operations)
            yield {"type": "tool_execution", "strategy": "serial"}
            tool_results = []
            async for result in self._execute_serial_tools(tool_calls, **kwargs):
                if result["type"] in ["tool_start", "tool_result", "tool_error"]:
                    yield result
                elif result["type"] == "tool_completed":
                    tool_results.append(result["llm_message"])

        # Yield final results
        yield {
            "type": "tool_results",
            "results": tool_results
        }



    def _is_tool_readonly(self, tool_name: str) -> bool:
        """Check if a tool is read-only (safe for concurrent execution)"""
        readonly_tools = {
            'read_file', 'read_many_files', 'ls', 'grep', 'glob',
            'web_fetch', 'web_search', 'git_status', 'git_log'
        }
        return tool_name in readonly_tools

    async def _execute_serial_tools(
        self,
        tool_calls: List[Dict[str, Any]],
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Execute tools one by one (simplified)"""
        
        for tool_call in tool_calls:
            try:
                # Yield tool start event
                yield {
                    "type": "tool_start",
                    "data": {
                        "call_id": tool_call.get("id", "unknown"),
                        "name": tool_call["name"],
                        "arguments": tool_call.get("arguments", {})
                    }
                }

                # Execute single tool directly
                tool_result, llm_message = await self._execute_single_tool_with_result(tool_call, **kwargs)

                # Yield tool result for CLI display
                yield {
                    "type": "tool_result", 
                    "data": {
                        "call_id": tool_call.get("id", "unknown"),
                        "name": tool_call["name"],
                        "result": tool_result.result if tool_result.success and isinstance(tool_result.result, dict) else str(tool_result.result or tool_result.error),
                        "success": tool_result.success
                    }
                }

                # Yield completed tool for message history
                yield {
                    "type": "tool_completed",
                    "llm_message": llm_message
                }

                # Check for abort signal
                if kwargs.get('abort_signal') and kwargs['abort_signal'].is_set():
                    break

            except Exception as e:
                error_msg = f"Error executing tool '{tool_call['name']}': {str(e)}"
                error_message = LLMMessage(
                    role="tool",
                    content=f"Error: {error_msg}",
                    tool_call_id=tool_call.get("id", "unknown")
                )

                yield {
                    "type": "tool_error",
                    "data": {
                        "call_id": tool_call.get("id", "unknown"),
                        "name": tool_call["name"],
                        "error": error_msg,
                        "success": False
                    }
                }

                yield {
                    "type": "tool_completed", 
                    "llm_message": error_message
                }

    async def _execute_single_tool_with_result(
        self,
        tool_call: Dict[str, Any],
        **kwargs
    ) -> tuple[ToolResult, LLMMessage]:
        """
        Execute a single tool and return both ToolResult and LLMMessage
        This is the main tool execution method that others can call
        """
        try:
            # Check for abort signal
            if kwargs.get('abort_signal') and kwargs['abort_signal'].is_set():
                cancelled_result = ToolResult(
                    call_id=tool_call.get("id", "unknown"),
                    content="",
                    error="Operation was cancelled",
                    success=False
                )
                cancelled_message = LLMMessage(
                    role="tool",
                    content="Operation was cancelled",
                    tool_call_id=tool_call.get("id", "unknown")
                )
                return cancelled_result, cancelled_message

            # Convert to ToolCall object
            tool_call_obj = ToolCall(
                call_id=tool_call.get("id", "unknown"),
                name=tool_call["name"],
                arguments=tool_call.get("arguments", {})
            )

            # Check for user confirmation if needed
            if hasattr(self, 'cli_console') and self.cli_console:
                tool = self.tool_registry.get_tool(tool_call["name"])
                if tool:
                    confirmation_details = await tool.get_confirmation_details(**tool_call.get("arguments", {}))
                    if confirmation_details:
                        confirmed = await self.cli_console.confirm_tool_call(tool_call_obj, tool)
                        if not confirmed:
                            # User cancelled
                            cancelled_result = ToolResult(
                                call_id=tool_call.get("id", "unknown"),
                                content="",
                                error="Tool execution was cancelled by user",
                                success=False
                            )
                            cancelled_message = LLMMessage(
                                role="tool",
                                content="Tool execution was cancelled by user",
                                tool_call_id=tool_call.get("id", "unknown")
                            )
                            return cancelled_result, cancelled_message

            # Execute tool
            results = await self.tool_executor.execute_tools([tool_call_obj], self.type)
            tool_result = results[0]

            # Create LLM message with clear success info
            if tool_result.success:
                if isinstance(tool_result.result, dict):
                    operation = tool_result.result.get('operation', '')
                    file_path = tool_result.result.get('file_path', '')

                    if operation == 'edit_file':
                        old_text = tool_result.result.get('old_text', '')
                        new_text = tool_result.result.get('new_text', '')
                        content = f"SUCCESS: File {file_path} edited successfully. Changed '{old_text}' to '{new_text}'. Task completed."
                    elif operation == 'write_file':
                        content = f"SUCCESS: File {file_path} written successfully. Task completed."
                    else:
                        content = tool_result.result.get('summary', str(tool_result.result))
                else:
                    content = str(tool_result.result) if tool_result.result is not None else "Operation completed successfully"
            else:
                content = f"Error: {tool_result.error}" if tool_result.error else "Tool execution failed"

            llm_message = LLMMessage(
                role="tool",
                content=content,
                tool_call_id=tool_call.get("id", "unknown")
            )

            return tool_result, llm_message

        except Exception as e:
            error_msg = f"Error executing tool '{tool_call['name']}': {str(e)}"
            error_result = ToolResult(
                call_id=tool_call.get("id", "unknown"),
                content="",
                error=error_msg,
                success=False
            )
            error_message = LLMMessage(
                role="tool",
                content=f"Error: {error_msg}",
                tool_call_id=tool_call.get("id", "unknown")
            )
            return error_result, error_message

    async def _execute_concurrent_tools(
        self,
        tool_calls: List[Dict[str, Any]],
        **kwargs
    ) -> List[LLMMessage]:
        """Execute multiple tools concurrently (for read-only tools)"""
        import asyncio

        # Create tasks for concurrent execution
        tasks = []
        for tool_call in tool_calls:
            task = asyncio.create_task(
                self._execute_single_tool(tool_call, **kwargs)
            )
            tasks.append(task)

        # Wait for all tasks to complete
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results and maintain order
        tool_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # Handle exception
                error_msg = f"Error executing tool '{tool_calls[i]['name']}': {str(result)}"
                tool_results.append(LLMMessage(
                    role="tool",
                    content=f"Error: {error_msg}",
                    tool_call_id=tool_calls[i].get("id", "unknown")
                ))
            else:
                tool_results.append(result)

        return tool_results







    async def _execute_single_tool(
        self,
        tool_call: Dict[str, Any],
        **kwargs
    ) -> LLMMessage:
        """
        Execute a single tool and return only the LLMMessage
        This is a convenience wrapper around _execute_single_tool_with_result
        """
        _, llm_message = await self._execute_single_tool_with_result(tool_call, **kwargs)
        return llm_message

    async def _execute_tool_directly(
        self,
        tool_call: Dict[str, Any],
        **kwargs
    ) -> ToolResult:
        """
        Execute a single tool and return only the ToolResult
        This is a convenience wrapper around _execute_single_tool_with_result
        """
        tool_result, _ = await self._execute_single_tool_with_result(tool_call, **kwargs)
        return tool_result


    
    def _find_tool(self, tool_name: str) -> Optional[BaseTool]:
        """Find a tool by name"""
        return self.tool_registry.get_tool(tool_name)
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Get agent capabilities"""
        return {
            "name": "Claude Code",
            "type": self.type,
            "description": "AI coding assistant with file operations and command execution",
            "features": [
                "Code analysis and writing",
                "File operations",
                "Command execution",
                "Project understanding",
                "Sub-agent delegation",
                "Context-aware responses",
                "Conversation history memory"
            ],
            "tools": [tool.name for tool in self.tools],
            "supports_streaming": True,
            "supports_sub_agents": True,
            "conversation_history": {
                "enabled": True,
                # "max_messages": self.max_history_messages,
                "current_messages": len(self.conversation_history),
                "summary": self.get_conversation_summary()
            }
        }
    
    def set_project_path(self, path: str):
        """Set the current project path"""
        if os.path.exists(path):
            self.project_path = path
            self._update_context()
        else:
            raise ValueError(f"Path does not exist: {path}")